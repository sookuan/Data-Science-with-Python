{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "469eccff-f2d9-47e6-b2e7-86b8401d8d9b",
   "metadata": {
    "id": "469eccff-f2d9-47e6-b2e7-86b8401d8d9b"
   },
   "source": [
    "<div>\n",
    "<img src=https://www.institutedata.com/wp-content/uploads/2019/10/iod_h_tp_primary_c.svg width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a81de7-1bfa-4e29-98d8-e98162bb7612",
   "metadata": {
    "id": "57a81de7-1bfa-4e29-98d8-e98162bb7612"
   },
   "source": [
    "# Lab 8.5 - Prompting Large Language Models\n",
    "\n",
    "In this lab we will practise prompting with a few Large Language Models (LLMs) using Groq (not to be confused with Grok). Groq is a platform that provides access to their custom-built AI hardware via APIs, allowing users to run open-source models such as Llama.\n",
    "\n",
    "We shall see that while LLMs are powerful tools, how you ask a question or frame a task can dramatically influence the results obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca332d1-638f-44c8-9f20-eab2f62bec2a",
   "metadata": {
    "id": "7ca332d1-638f-44c8-9f20-eab2f62bec2a"
   },
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111d6485-4dd7-47d7-a0f4-eba2a6cb3719",
   "metadata": {
    "id": "111d6485-4dd7-47d7-a0f4-eba2a6cb3719"
   },
   "source": [
    "Step 1: Sign up for a free Groq account at https://console.groq.com/home .\n",
    "\n",
    "Step 2: Create a new API key at https://console.groq.com/keys. Copy-paste it into an empty text file called 'groq_key.txt'.\n",
    "\n",
    "Running the next cell will then read in this key and assign it to the variable `groq_key`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56441b82-ddc5-46e9-9583-878e8a807b7f",
   "metadata": {
    "id": "56441b82-ddc5-46e9-9583-878e8a807b7f"
   },
   "outputs": [],
   "source": [
    "groqfilename = r'groq_key.txt' # this file contains a single line containing your Groq API key only\n",
    "try:\n",
    "    with open(groqfilename, 'r') as f:\n",
    "        groq_key = f.read().strip()\n",
    "except FileNotFoundError:\n",
    "    print(f\"'{groqfilename}' file not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "606b6102-4a13-44bb-9324-d1f9aff8ac88",
   "metadata": {
    "id": "606b6102-4a13-44bb-9324-d1f9aff8ac88"
   },
   "outputs": [],
   "source": [
    "#!pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd0b4dfc-47a9-46f9-ae1a-6241b07ba41c",
   "metadata": {
    "id": "dd0b4dfc-47a9-46f9-ae1a-6241b07ba41c"
   },
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "import requests\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eef37e2-24ba-4e6d-a8ce-87ade1ef1227",
   "metadata": {
    "id": "1eef37e2-24ba-4e6d-a8ce-87ade1ef1227"
   },
   "source": [
    "First create an instance of the Groq client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54fec468-0ae0-42e1-ab8c-087b97c9818b",
   "metadata": {
    "id": "54fec468-0ae0-42e1-ab8c-087b97c9818b"
   },
   "outputs": [],
   "source": [
    "client = Groq(api_key=groq_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a097cd77-7a8d-4502-ae6e-607782706b26",
   "metadata": {
    "id": "a097cd77-7a8d-4502-ae6e-607782706b26"
   },
   "source": [
    "The following code shows what models are currently accessible through Groq. `context_window` refers to the size of memory (in tokens) during a session and `max_completion_tokens` is the maximum number of tokens that are generated in an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33fc8058-c9c7-49b3-b294-cb4e12de20f4",
   "metadata": {
    "id": "33fc8058-c9c7-49b3-b294-cb4e12de20f4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>object</th>\n",
       "      <th>created</th>\n",
       "      <th>owned_by</th>\n",
       "      <th>active</th>\n",
       "      <th>context_window</th>\n",
       "      <th>public_apps</th>\n",
       "      <th>max_completion_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>openai/gpt-oss-120b</td>\n",
       "      <td>model</td>\n",
       "      <td>1754408224</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>65536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>openai/gpt-oss-20b</td>\n",
       "      <td>model</td>\n",
       "      <td>1754407957</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>65536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>moonshotai/kimi-k2-instruct</td>\n",
       "      <td>model</td>\n",
       "      <td>1752435491</td>\n",
       "      <td>Moonshot AI</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>16384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>meta-llama/llama-prompt-guard-2-86m</td>\n",
       "      <td>model</td>\n",
       "      <td>1748632165</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>512</td>\n",
       "      <td>None</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>meta-llama/llama-prompt-guard-2-22m</td>\n",
       "      <td>model</td>\n",
       "      <td>1748632101</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>512</td>\n",
       "      <td>None</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>qwen/qwen3-32b</td>\n",
       "      <td>model</td>\n",
       "      <td>1748396646</td>\n",
       "      <td>Alibaba Cloud</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>40960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>meta-llama/llama-guard-4-12b</td>\n",
       "      <td>model</td>\n",
       "      <td>1746743847</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>meta-llama/llama-4-maverick-17b-128e-instruct</td>\n",
       "      <td>model</td>\n",
       "      <td>1743877158</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>meta-llama/llama-4-scout-17b-16e-instruct</td>\n",
       "      <td>model</td>\n",
       "      <td>1743874824</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>compound-beta-mini</td>\n",
       "      <td>model</td>\n",
       "      <td>1742953279</td>\n",
       "      <td>Groq</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>compound-beta</td>\n",
       "      <td>model</td>\n",
       "      <td>1740880017</td>\n",
       "      <td>Groq</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>playai-tts-arabic</td>\n",
       "      <td>model</td>\n",
       "      <td>1740682783</td>\n",
       "      <td>PlayAI</td>\n",
       "      <td>True</td>\n",
       "      <td>8192</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>playai-tts</td>\n",
       "      <td>model</td>\n",
       "      <td>1740682771</td>\n",
       "      <td>PlayAI</td>\n",
       "      <td>True</td>\n",
       "      <td>8192</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deepseek-r1-distill-llama-70b</td>\n",
       "      <td>model</td>\n",
       "      <td>1737924940</td>\n",
       "      <td>DeepSeek / Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>131072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>allam-2-7b</td>\n",
       "      <td>model</td>\n",
       "      <td>1737672203</td>\n",
       "      <td>SDAIA</td>\n",
       "      <td>True</td>\n",
       "      <td>4096</td>\n",
       "      <td>None</td>\n",
       "      <td>4096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>llama-3.3-70b-versatile</td>\n",
       "      <td>model</td>\n",
       "      <td>1733447754</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>32768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>whisper-large-v3-turbo</td>\n",
       "      <td>model</td>\n",
       "      <td>1728413088</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>True</td>\n",
       "      <td>448</td>\n",
       "      <td>None</td>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gemma2-9b-it</td>\n",
       "      <td>model</td>\n",
       "      <td>1693721698</td>\n",
       "      <td>Google</td>\n",
       "      <td>True</td>\n",
       "      <td>8192</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>llama3-8b-8192</td>\n",
       "      <td>model</td>\n",
       "      <td>1693721698</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>8192</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>whisper-large-v3</td>\n",
       "      <td>model</td>\n",
       "      <td>1693721698</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>True</td>\n",
       "      <td>448</td>\n",
       "      <td>None</td>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>llama-3.1-8b-instant</td>\n",
       "      <td>model</td>\n",
       "      <td>1693721698</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>131072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>llama3-70b-8192</td>\n",
       "      <td>model</td>\n",
       "      <td>1693721698</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>8192</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               id object     created  \\\n",
       "11                            openai/gpt-oss-120b  model  1754408224   \n",
       "21                             openai/gpt-oss-20b  model  1754407957   \n",
       "0                     moonshotai/kimi-k2-instruct  model  1752435491   \n",
       "5             meta-llama/llama-prompt-guard-2-86m  model  1748632165   \n",
       "8             meta-llama/llama-prompt-guard-2-22m  model  1748632101   \n",
       "19                                 qwen/qwen3-32b  model  1748396646   \n",
       "9                    meta-llama/llama-guard-4-12b  model  1746743847   \n",
       "15  meta-llama/llama-4-maverick-17b-128e-instruct  model  1743877158   \n",
       "16      meta-llama/llama-4-scout-17b-16e-instruct  model  1743874824   \n",
       "17                             compound-beta-mini  model  1742953279   \n",
       "1                                   compound-beta  model  1740880017   \n",
       "2                               playai-tts-arabic  model  1740682783   \n",
       "14                                     playai-tts  model  1740682771   \n",
       "4                   deepseek-r1-distill-llama-70b  model  1737924940   \n",
       "18                                     allam-2-7b  model  1737672203   \n",
       "12                        llama-3.3-70b-versatile  model  1733447754   \n",
       "10                         whisper-large-v3-turbo  model  1728413088   \n",
       "3                                    gemma2-9b-it  model  1693721698   \n",
       "13                                 llama3-8b-8192  model  1693721698   \n",
       "6                                whisper-large-v3  model  1693721698   \n",
       "7                            llama-3.1-8b-instant  model  1693721698   \n",
       "20                                llama3-70b-8192  model  1693721698   \n",
       "\n",
       "           owned_by  active  context_window public_apps  max_completion_tokens  \n",
       "11           OpenAI    True          131072        None                  65536  \n",
       "21           OpenAI    True          131072        None                  65536  \n",
       "0       Moonshot AI    True          131072        None                  16384  \n",
       "5              Meta    True             512        None                    512  \n",
       "8              Meta    True             512        None                    512  \n",
       "19    Alibaba Cloud    True          131072        None                  40960  \n",
       "9              Meta    True          131072        None                   1024  \n",
       "15             Meta    True          131072        None                   8192  \n",
       "16             Meta    True          131072        None                   8192  \n",
       "17             Groq    True          131072        None                   8192  \n",
       "1              Groq    True          131072        None                   8192  \n",
       "2            PlayAI    True            8192        None                   8192  \n",
       "14           PlayAI    True            8192        None                   8192  \n",
       "4   DeepSeek / Meta    True          131072        None                 131072  \n",
       "18            SDAIA    True            4096        None                   4096  \n",
       "12             Meta    True          131072        None                  32768  \n",
       "10           OpenAI    True             448        None                    448  \n",
       "3            Google    True            8192        None                   8192  \n",
       "13             Meta    True            8192        None                   8192  \n",
       "6            OpenAI    True             448        None                    448  \n",
       "7              Meta    True          131072        None                 131072  \n",
       "20             Meta    True            8192        None                   8192  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://api.groq.com/openai/v1/models\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {groq_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "pd.DataFrame(response.json()['data']).sort_values(['created'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b438489-c102-434b-872d-e807169deef6",
   "metadata": {
    "id": "2b438489-c102-434b-872d-e807169deef6"
   },
   "source": [
    "The Groq client object enables interaction with the Groq REST API and a chat completion request is made via the client.chat.completions.create method.\n",
    "\n",
    "The most important arguments of the client.chat.completions.create method are the following:\n",
    "* messages: a list of messages (dictionary form) that make up the conversation to date\n",
    "* model: a string indicating which model to use (see [list of models](https://console.groq.com/docs/models))\n",
    "* max_completion_tokens: the maximum number of tokens that are generated in the chat completion\n",
    "* response_format: setting this to `{ \"type\": \"json_object\" }` enables JSON output\n",
    "* seed: sample deterministically as best as possible, though identical outputs each time are not guaranteed\n",
    "* temperature: between 0 and 2 where higher values like 0.8 make the output more random (creative) and values like 0.2 are more focused and deterministic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99216392-acc3-4a00-826b-c166a1e52534",
   "metadata": {
    "id": "99216392-acc3-4a00-826b-c166a1e52534"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method create in module groq.resources.chat.completions:\n",
      "\n",
      "create(*, messages: 'Iterable[ChatCompletionMessageParam]', model: \"Union[str, Literal['gemma2-9b-it', 'llama-3.3-70b-versatile', 'llama-3.1-8b-instant', 'llama-guard-3-8b', 'llama3-70b-8192', 'llama3-8b-8192']]\", exclude_domains: 'Optional[List[str]] | NotGiven' = NOT_GIVEN, frequency_penalty: 'Optional[float] | NotGiven' = NOT_GIVEN, function_call: 'Optional[completion_create_params.FunctionCall] | NotGiven' = NOT_GIVEN, functions: 'Optional[Iterable[completion_create_params.Function]] | NotGiven' = NOT_GIVEN, include_domains: 'Optional[List[str]] | NotGiven' = NOT_GIVEN, include_reasoning: 'Optional[bool] | NotGiven' = NOT_GIVEN, logit_bias: 'Optional[Dict[str, int]] | NotGiven' = NOT_GIVEN, logprobs: 'Optional[bool] | NotGiven' = NOT_GIVEN, max_completion_tokens: 'Optional[int] | NotGiven' = NOT_GIVEN, max_tokens: 'Optional[int] | NotGiven' = NOT_GIVEN, metadata: 'Optional[Dict[str, str]] | NotGiven' = NOT_GIVEN, n: 'Optional[int] | NotGiven' = NOT_GIVEN, parallel_tool_calls: 'Optional[bool] | NotGiven' = NOT_GIVEN, presence_penalty: 'Optional[float] | NotGiven' = NOT_GIVEN, reasoning_effort: \"Optional[Literal['none', 'default', 'low', 'medium', 'high']] | NotGiven\" = NOT_GIVEN, reasoning_format: \"Optional[Literal['hidden', 'raw', 'parsed']] | NotGiven\" = NOT_GIVEN, response_format: 'Optional[completion_create_params.ResponseFormat] | NotGiven' = NOT_GIVEN, search_settings: 'Optional[completion_create_params.SearchSettings] | NotGiven' = NOT_GIVEN, seed: 'Optional[int] | NotGiven' = NOT_GIVEN, service_tier: \"Optional[Literal['auto', 'on_demand', 'flex', 'performance']] | NotGiven\" = NOT_GIVEN, stop: 'Union[Optional[str], List[str], None] | NotGiven' = NOT_GIVEN, store: 'Optional[bool] | NotGiven' = NOT_GIVEN, stream: 'Optional[Literal[False]] | Literal[True] | NotGiven' = NOT_GIVEN, temperature: 'Optional[float] | NotGiven' = NOT_GIVEN, tool_choice: 'Optional[ChatCompletionToolChoiceOptionParam] | NotGiven' = NOT_GIVEN, tools: 'Optional[Iterable[ChatCompletionToolParam]] | NotGiven' = NOT_GIVEN, top_logprobs: 'Optional[int] | NotGiven' = NOT_GIVEN, top_p: 'Optional[float] | NotGiven' = NOT_GIVEN, user: 'Optional[str] | NotGiven' = NOT_GIVEN, extra_headers: 'Headers | None' = None, extra_query: 'Query | None' = None, extra_body: 'Body | None' = None, timeout: 'float | httpx.Timeout | None | NotGiven' = NOT_GIVEN) -> 'ChatCompletion | Stream[ChatCompletionChunk]' method of groq.resources.chat.completions.Completions instance\n",
      "    Creates a model response for the given chat conversation.\n",
      "    \n",
      "    Args:\n",
      "      messages: A list of messages comprising the conversation so far.\n",
      "    \n",
      "      model: ID of the model to use. For details on which models are compatible with the Chat\n",
      "          API, see available [models](https://console.groq.com/docs/models)\n",
      "    \n",
      "      exclude_domains: Deprecated: Use search_settings.exclude_domains instead. A list of domains to\n",
      "          exclude from the search results when the model uses a web search tool.\n",
      "    \n",
      "      frequency_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n",
      "          existing frequency in the text so far, decreasing the model's likelihood to\n",
      "          repeat the same line verbatim.\n",
      "    \n",
      "      function_call: Deprecated in favor of `tool_choice`.\n",
      "    \n",
      "          Controls which (if any) function is called by the model. `none` means the model\n",
      "          will not call a function and instead generates a message. `auto` means the model\n",
      "          can pick between generating a message or calling a function. Specifying a\n",
      "          particular function via `{\"name\": \"my_function\"}` forces the model to call that\n",
      "          function.\n",
      "    \n",
      "          `none` is the default when no functions are present. `auto` is the default if\n",
      "          functions are present.\n",
      "    \n",
      "      functions: Deprecated in favor of `tools`.\n",
      "    \n",
      "          A list of functions the model may generate JSON inputs for.\n",
      "    \n",
      "      include_domains: Deprecated: Use search_settings.include_domains instead. A list of domains to\n",
      "          include in the search results when the model uses a web search tool.\n",
      "    \n",
      "      include_reasoning: Whether to include reasoning in the response. If true, the response will include\n",
      "          a `reasoning` field. If false, the model's reasoning will not be included in the\n",
      "          response. This field is mutually exclusive with `reasoning_format`.\n",
      "    \n",
      "      logit_bias: This is not yet supported by any of our models. Modify the likelihood of\n",
      "          specified tokens appearing in the completion.\n",
      "    \n",
      "      logprobs: This is not yet supported by any of our models. Whether to return log\n",
      "          probabilities of the output tokens or not. If true, returns the log\n",
      "          probabilities of each output token returned in the `content` of `message`.\n",
      "    \n",
      "      max_completion_tokens: The maximum number of tokens that can be generated in the chat completion. The\n",
      "          total length of input tokens and generated tokens is limited by the model's\n",
      "          context length.\n",
      "    \n",
      "      max_tokens: Deprecated in favor of `max_completion_tokens`. The maximum number of tokens\n",
      "          that can be generated in the chat completion. The total length of input tokens\n",
      "          and generated tokens is limited by the model's context length.\n",
      "    \n",
      "      metadata: This parameter is not currently supported.\n",
      "    \n",
      "      n: How many chat completion choices to generate for each input message. Note that\n",
      "          the current moment, only n=1 is supported. Other values will result in a 400\n",
      "          response.\n",
      "    \n",
      "      parallel_tool_calls: Whether to enable parallel function calling during tool use.\n",
      "    \n",
      "      presence_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on\n",
      "          whether they appear in the text so far, increasing the model's likelihood to\n",
      "          talk about new topics.\n",
      "    \n",
      "      reasoning_effort: this field is only available for qwen3 models. Set to 'none' to disable\n",
      "          reasoning. Set to 'default' or null to let Qwen reason.\n",
      "    \n",
      "      reasoning_format: Specifies how to output reasoning tokens This field is mutually exclusive with\n",
      "          `include_reasoning`.\n",
      "    \n",
      "      response_format: An object specifying the format that the model must output. Setting to\n",
      "          `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables Structured Outputs\n",
      "          which ensures the model will match your supplied JSON schema. `json_schema`\n",
      "          response format is only available on\n",
      "          [supported models](https://console.groq.com/docs/structured-outputs#supported-models).\n",
      "          Setting to `{ \"type\": \"json_object\" }` enables the older JSON mode, which\n",
      "          ensures the message the model generates is valid JSON. Using `json_schema` is\n",
      "          preferred for models that support it.\n",
      "    \n",
      "      search_settings: Settings for web search functionality when the model uses a web search tool.\n",
      "    \n",
      "      seed: If specified, our system will make a best effort to sample deterministically,\n",
      "          such that repeated requests with the same `seed` and parameters should return\n",
      "          the same result. Determinism is not guaranteed, and you should refer to the\n",
      "          `system_fingerprint` response parameter to monitor changes in the backend.\n",
      "    \n",
      "      service_tier: The service tier to use for the request. Defaults to `on_demand`.\n",
      "    \n",
      "          - `auto` will automatically select the highest tier available within the rate\n",
      "            limits of your organization.\n",
      "          - `flex` uses the flex tier, which will succeed or fail quickly.\n",
      "    \n",
      "      stop: Up to 4 sequences where the API will stop generating further tokens. The\n",
      "          returned text will not contain the stop sequence.\n",
      "    \n",
      "      store: This parameter is not currently supported.\n",
      "    \n",
      "      stream: If set, partial message deltas will be sent. Tokens will be sent as data-only\n",
      "          [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)\n",
      "          as they become available, with the stream terminated by a `data: [DONE]`\n",
      "          message. [Example code](/docs/text-chat#streaming-a-chat-completion).\n",
      "    \n",
      "      temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n",
      "          make the output more random, while lower values like 0.2 will make it more\n",
      "          focused and deterministic. We generally recommend altering this or top_p but not\n",
      "          both.\n",
      "    \n",
      "      tool_choice: Controls which (if any) tool is called by the model. `none` means the model will\n",
      "          not call any tool and instead generates a message. `auto` means the model can\n",
      "          pick between generating a message or calling one or more tools. `required` means\n",
      "          the model must call one or more tools. Specifying a particular tool via\n",
      "          `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}` forces the model to\n",
      "          call that tool.\n",
      "    \n",
      "          `none` is the default when no tools are present. `auto` is the default if tools\n",
      "          are present.\n",
      "    \n",
      "      tools: A list of tools the model may call. Currently, only functions are supported as a\n",
      "          tool. Use this to provide a list of functions the model may generate JSON inputs\n",
      "          for. A max of 128 functions are supported.\n",
      "    \n",
      "      top_logprobs: This is not yet supported by any of our models. An integer between 0 and 20\n",
      "          specifying the number of most likely tokens to return at each token position,\n",
      "          each with an associated log probability. `logprobs` must be set to `true` if\n",
      "          this parameter is used.\n",
      "    \n",
      "      top_p: An alternative to sampling with temperature, called nucleus sampling, where the\n",
      "          model considers the results of the tokens with top_p probability mass. So 0.1\n",
      "          means only the tokens comprising the top 10% probability mass are considered. We\n",
      "          generally recommend altering this or temperature but not both.\n",
      "    \n",
      "      user: A unique identifier representing your end-user, which can help us monitor and\n",
      "          detect abuse.\n",
      "    \n",
      "      extra_headers: Send extra headers\n",
      "    \n",
      "      extra_query: Add additional query parameters to the request\n",
      "    \n",
      "      extra_body: Add additional JSON properties to the request\n",
      "    \n",
      "      timeout: Override the client-level default timeout for this request, in seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(client.chat.completions.create)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d17d3c7-d7b5-47ed-b514-40b0a3e577b7",
   "metadata": {
    "id": "4d17d3c7-d7b5-47ed-b514-40b0a3e577b7"
   },
   "source": [
    "As a first example, note how the messages input is given as a list of a dictionaries with `role` and `content` keys. This is in a ChatML format recognised by many LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cfd33fb-15fc-409d-a5a9-e8770885df81",
   "metadata": {
    "id": "9cfd33fb-15fc-409d-a5a9-e8770885df81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large language models are artificial intelligence (AI) systems that process and generate human-like language. Here's a brief overview of how they work:\n",
      "\n",
      "1. **Training**: These models are trained on massive amounts of text data, often from the internet, books, or other sources.\n",
      "2. **Pattern recognition**: The models use complex algorithms to recognize patterns in the training data, such as grammar, syntax, and word relationships.\n",
      "3. **Neural network**: The models are built using neural networks, which are composed of layers of interconnected nodes (neurons) that process and transform the input data.\n",
      "4. **Tokenization**: When you input text, it's broken down into individual tokens (words, characters, or subwords).\n",
      "5. **Generation**: The model uses the patterns learned during training to predict the next token, given the context of the input text.\n",
      "6. **Iteration**: This process is repeated, with the model generating text one token at a time, until a complete response is formed.\n",
      "\n",
      "By leveraging these techniques, large language models can understand and generate coherent, context-dependent text, allowing them to perform tasks like answering questions, translating languages, and even creating original content.\n"
     ]
    }
   ],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {   \"role\": \"system\", # sets the persona of the model\n",
    "            \"content\": \"You are a helpful assistant.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", # what the user wants the assistant to do\n",
    "            \"content\": \"Explain briefly how large language models work\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728b60c2-4300-4338-b325-c3ea876c1afe",
   "metadata": {
    "id": "728b60c2-4300-4338-b325-c3ea876c1afe"
   },
   "source": [
    "The output is in Markdown format so the following line formats this text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88845bb3-4a3e-403a-93ab-439c46aa1832",
   "metadata": {
    "id": "88845bb3-4a3e-403a-93ab-439c46aa1832"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Large language models are artificial intelligence (AI) systems that process and generate human-like language. Here's a brief overview of how they work:\n",
       "\n",
       "1. **Training**: These models are trained on massive amounts of text data, often from the internet, books, or other sources.\n",
       "2. **Pattern recognition**: The models use complex algorithms to recognize patterns in the training data, such as grammar, syntax, and word relationships.\n",
       "3. **Neural network**: The models are built using neural networks, which are composed of layers of interconnected nodes (neurons) that process and transform the input data.\n",
       "4. **Tokenization**: When you input text, it's broken down into individual tokens (words, characters, or subwords).\n",
       "5. **Generation**: The model uses the patterns learned during training to predict the next token, given the context of the input text.\n",
       "6. **Iteration**: This process is repeated, with the model generating text one token at a time, until a complete response is formed.\n",
       "\n",
       "By leveraging these techniques, large language models can understand and generate coherent, context-dependent text, allowing them to perform tasks like answering questions, translating languages, and even creating original content."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c8c97b-1711-4355-b108-86bed769c109",
   "metadata": {
    "id": "e1c8c97b-1711-4355-b108-86bed769c109"
   },
   "source": [
    "## Text summarisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930cdd6b-8d44-4b3e-b161-0ebcd4600bc4",
   "metadata": {
    "id": "930cdd6b-8d44-4b3e-b161-0ebcd4600bc4"
   },
   "source": [
    "We start with a llama3-8b-8192, a model using just over 8 billion parameters with at most 8192 tokens produced as output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b56002-d85b-44d6-a2d0-4b1513eeb4f2",
   "metadata": {
    "id": "b7b56002-d85b-44d6-a2d0-4b1513eeb4f2"
   },
   "source": [
    "Here is an article to be summarised from the [cnn_dailymail](https://huggingface.co/datasets/abisee/cnn_dailymail) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c426f9f-c780-4ca6-b913-9b5c63b8bb29",
   "metadata": {
    "id": "7c426f9f-c780-4ca6-b913-9b5c63b8bb29"
   },
   "outputs": [],
   "source": [
    "story = \"\"\"\n",
    "SAN FRANCISCO, California (CNN) -- A magnitude 4.2 earthquake shook the San Francisco area Friday at 4:42 a.m. PT (7:42 a.m. ET), the U.S. Geological Survey reported. The quake left about 2,000 customers without power, said David Eisenhower, a spokesman for Pacific Gas and Light. Under the USGS classification, a magnitude 4.2 earthquake is considered \"light,\" which it says usually causes minimal damage. \"We had quite a spike in calls, mostly calls of inquiry, none of any injury, none of any damage that was reported,\" said Capt. Al Casciato of the San Francisco police. \"It was fairly mild.\" Watch police describe concerned calls immediately after the quake » . The quake was centered about two miles east-northeast of Oakland, at a depth of 3.6 miles, the USGS said. Oakland is just east of San Francisco, across San Francisco Bay. An Oakland police dispatcher told CNN the quake set off alarms at people's homes. The shaking lasted about 50 seconds, said CNN meteorologist Chad Myers. According to the USGS, magnitude 4.2 quakes are felt indoors and may break dishes and windows and overturn unstable objects. Pendulum clocks may stop.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9754257a-74ff-4747-a18e-7ebfbc82639c",
   "metadata": {
    "id": "9754257a-74ff-4747-a18e-7ebfbc82639c"
   },
   "source": [
    "**Exercise:**\n",
    "Summarise the story text using the following three prompts. Use the format given above but here there is no need to set the persona (i.e. only include one dictionary in the messages list when calling `client.chat.completions.create`.) Comment on any differences.\n",
    "\n",
    "1) \"Summarise the following article in 3 sentences.\"\n",
    "\n",
    "2) \"Give me a TL;DR of this text.\"\n",
    "\n",
    "3) \"What's the key takeaway here?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c372155-bdde-45a4-a184-fce4c4e8034c",
   "metadata": {
    "id": "9c372155-bdde-45a4-a184-fce4c4e8034c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarise the following article in 3 sentences.  \n",
      " Here is a summary of the article in 3 sentences:\n",
      "\n",
      "A magnitude 4.2 earthquake struck the San Francisco area on Friday morning, causing minimal damage and only temporarily leaving about 2,000 customers without power. The quake was considered \"light\" and was felt for about 50 seconds, with no reported injuries or significant damage. According to the USGS, a magnitude 4.2 quake can cause items to fall and break, but overall, the impact was described as \"fairly mild\" by authorities.\n",
      "Give me a TL;DR of this text.  \n",
      " Here is a TL;DR (Too Long; Didn't Read) summary of the text:\n",
      "\n",
      "A 4.2-magnitude earthquake struck the San Francisco area at 4:42am on Friday, causing about 2,000 power outages and triggering alarm calls. The quake was shallow (3.6 miles deep) and centered near Oakland, about 2 miles east of San Francisco. The USGS considers 4.2 magnitudes as \"light\" and minimal damage was reported.\n",
      "What's the key takeaway here? \n",
      " The key takeaway is that a magnitude 4.2 earthquake struck the San Francisco area, causing about 2,000 customers to lose power and shaking for about 50 seconds, but no reports of injury or significant damage were reported, as it was considered a \"light\" earthquake.\n"
     ]
    }
   ],
   "source": [
    "prompts = [\"Summarise the following article in 3 sentences. \", \"Give me a TL;DR of this text. \", \"What's the key takeaway here?\"]\n",
    "#content will be p + story for p in prompts\n",
    "\n",
    "# ANSWER\n",
    "for p in prompts:\n",
    "    response = client.chat.completions.create(\n",
    "        model = \"llama3-8b-8192\",\n",
    "        messages = [{\"role\": \"user\", \"content\": p+story}]\n",
    "    )\n",
    "    \n",
    "    print(p, \"\\n\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3634dd68-a9c1-42a2-a1ef-eb1487cbf9df",
   "metadata": {
    "id": "3634dd68-a9c1-42a2-a1ef-eb1487cbf9df"
   },
   "source": [
    "Run the above code again below and note that the answers may differ. This is due to the probabilistic nature of LLM token generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7dffda3-fb3b-4555-b6c6-9bbc33248c14",
   "metadata": {
    "id": "f7dffda3-fb3b-4555-b6c6-9bbc33248c14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarise the following article in 3 sentences.  \n",
      " Here is a summary of the article in 3 sentences:\n",
      "\n",
      "A magnitude 4.2 earthquake struck the San Francisco area at 4:42am PT, causing minimal damage and no reported injuries. The quake was centered 2 miles east-northeast of Oakland, just east of San Francisco, and was described as \"fairly mild\" by authorities. The earthquake left about 2,000 customers without power, but overall damage was minimal, with only a few reports of knocked-over objects and alarm clocks set off in homes.\n",
      "Give me a TL;DR of this text.  \n",
      " Here is a brief summary of the text:\n",
      "\n",
      "A magnitude 4.2 earthquake struck the San Francisco area at 4:42am, causing minimal damage and no reported injuries. About 2,000 customers lost power, but most call-ins were inquiries rather than reports of damage. The quake was centered near Oakland, lasted around 50 seconds, and caused some objects to fall or be disturbed.\n",
      "What's the key takeaway here? \n",
      " The key takeaway is that a magnitude 4.2 earthquake struck the San Francisco area on Friday, causing minimal damage and no reported injuries, although it did leave around 2,000 customers without power. The quake was considered \"light\" by the USGS and was felt indoors, but did not cause significant disruption.\n"
     ]
    }
   ],
   "source": [
    "# ANSWER\n",
    "prompts = [\"Summarise the following article in 3 sentences. \", \"Give me a TL;DR of this text. \", \"What's the key takeaway here?\"]\n",
    "#content will be p + story for p in prompts\n",
    "\n",
    "# ANSWER\n",
    "for p in prompts:\n",
    "    response = client.chat.completions.create(\n",
    "        model = \"llama3-8b-8192\",\n",
    "        messages = [{\"role\": \"user\", \"content\": p+story}]\n",
    "    )\n",
    "    \n",
    "    print(p, \"\\n\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b4a80d-acb8-4099-b8e2-dba4a372369f",
   "metadata": {
    "id": "97b4a80d-acb8-4099-b8e2-dba4a372369f"
   },
   "source": [
    "## Text completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0be4be-e054-4b9c-ad38-843c97d3afaf",
   "metadata": {
    "id": "2b0be4be-e054-4b9c-ad38-843c97d3afaf"
   },
   "source": [
    "**Exercise**: In this section adjust the `max_completion_tokens` and `temperature` settings below to obtain different responses. Show some examples with the prompt \"Continue the story: It was a great time to be alive\" with the model \"llama-3.1-8b-instant\".\n",
    "\n",
    "* max_completion_tokens - the maximum number of tokens to generate. Note that longer words are made of multiple tokens (set to 200 and 500)\n",
    "* temperature (positive number) - the higher the number the more random (creative) the output (set to 0.2, 0.8, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "50207704-94be-4309-8a82-dc1d22a063ee",
   "metadata": {
    "id": "50207704-94be-4309-8a82-dc1d22a063ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As I walked through the bustling streets of the city, I couldn't help but feel a sense of excitement and joy that seemed to permeate the air. Everywhere I looked, there were people smiling, laughing, and soaking up the sun. The atmosphere was electric, and I found myself grinning from ear to ear as I took it all in.\n",
      "\n",
      "It was the summer of 1967, and I was 25 years old. I had just returned from a few months traveling the world, and I was settled back into my routine as a young journalist. I had always been passionate about music, and I felt like I was at the epicenter of it all.\n",
      "\n",
      "Every night, I would head to the legendary Fillmore Auditorium in San Francisco, where the likes of Janis Joplin, Jimi Hendrix, and The Grateful Dead would take the stage. The music was like nothing I had ever heard before - raw, emotional, and deeply connected to the times we were living\n"
     ]
    }
   ],
   "source": [
    "# ANSWER (set max_completion_tokens=200, do not have a temperature setting)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model = \"llama-3.1-8b-instant\",\n",
    "    messages = [{\"role\": \"user\", \"content\": \"Continue the story: It was a great time to be alive\"}],\n",
    "    max_completion_tokens = 200\n",
    ")\n",
    "    \n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63235fcc-3a61-4829-998b-37f2f104322b",
   "metadata": {
    "id": "63235fcc-3a61-4829-998b-37f2f104322b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a great time to be alive, with the world at the brink of a new era of technological advancement. The air was electric with the hum of innovation, as humans reached for the stars and made giant leaps towards a brighter future.\n",
      "\n",
      "Ava sat on the edge of her bed, her fingers dancing across the sleek surface of her holographic screen as she scrolled through the latest news feeds. She watched in awe as a new breakthrough in space exploration was announced, marking the first time humans had successfully established a colony on Mars.\n",
      "\n",
      "Her city, New Eden, pulsed with excitement as the news spread like wildfire. People poured out of their homes and apartments, gathering in the city square to celebrate the occasion. Ava joined the throngs, her eyes shining with wonder as she watched the Martian colonists being honored on the massive digital screens that lined the square.\n",
      "\n",
      "As she gazed up at the screens, Ava felt a tap on her shoulder. She turned to see her best friend, Jax, grinning at her from ear to ear.\n",
      "\n",
      "\"Dude, have you seen the specs on that new quantum computer?\" Jax asked, his voice barely above a whisper. \"It's like, light-years ahead of anything we've ever seen. We can finally start exploring the mysteries of dark matter!\"\n",
      "\n",
      "Ava's eyes widened in excitement as she turned to follow Jax to the city's innovation hub. She knew that this was just the beginning – the era of human enlightenment was about to get a whole lot brighter.\n",
      "\n",
      "But as they walked, Ava couldn't shake off the feeling that there was something more to this revolution than met the eye. Something that had been hiding in the shadows, waiting to emerge. She didn't know what it was, but she felt it in her bones – a sense of impending change that would shake the very foundations of their world.\n",
      "\n",
      "And so, with the city lights shining brightly behind her, Ava set out on a journey to uncover the secrets that lay beyond the horizon. Little did she know, the greatest adventure of her life was just beginning...\n"
     ]
    }
   ],
   "source": [
    "# ANSWER (set max_completion_tokens=500, do not have a temperature setting)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model = \"llama-3.1-8b-instant\",\n",
    "    messages = [{\"role\": \"user\", \"content\": \"Continue the story: It was a great time to be alive\"}],\n",
    "    max_completion_tokens = 500\n",
    ")\n",
    "    \n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ac91a1a-4f55-4531-bb5b-cdb436a87e50",
   "metadata": {
    "id": "8ac91a1a-4f55-4531-bb5b-cdb436a87e50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a great time to be alive, and for Emily, it was especially so. The year was 1969, and the world was on the cusp of a revolution. The Beatles had just released their iconic album \"Abbey Road,\" and the music scene was buzzing with the sounds of Woodstock and the Summer of Love.\n",
      "\n",
      "Emily, a free-spirited 22-year-old, was right in the middle of it all. She had just graduated from college with a degree in art, and was working as a waitress at a trendy coffee shop in the Haight-Ashbury neighborhood of San Francisco. Her days were filled with the sounds of Jimi Hendrix and Janis Joplin, and her nights were spent dancing at the Fillmore with her friends.\n",
      "\n",
      "One evening, as she was closing up the coffee shop, Emily met a young man named Max. He was a musician, with a guitar slung over his shoulder and a mop of curly hair that fell across his forehead. He had a charming smile and a quick wit, and Emily was immediately drawn to him.\n",
      "\n",
      "As they talked, Emily learned that Max was part of a new band that was just starting to make waves in the San Francisco music scene. They were called \"The Electric Storm,\" and they played a fusion of rock, blues, and folk music that was unlike anything Emily had ever heard before.\n",
      "\n",
      "Max asked Emily if she wanted to come see them play at a small club in the Mission District, and she agreed. As they walked to the club, Emily couldn't help but feel a sense of excitement and possibility. She had a feeling that this was going to be a night to remember.\n",
      "\n",
      "When they arrived at the club, Emily was struck by the energy of the crowd. The room was packed with people of all ages and backgrounds, all united by their love of music. Max introduced Emily to the rest of the band, and they welcomed her with open arms.\n",
      "\n",
      "As the music began, Emily felt herself getting swept up in the energy of the performance. The Electric Storm was more than just a band - they were a community, a family of musicians who had come together to create something new and beautiful.\n",
      "\n",
      "As the night wore on, Emily found herself dancing in the aisle, lost in the music and the moment. She felt a sense of freedom and joy that she had never experienced before, and she knew that she would never forget this night.\n",
      "\n",
      "As the music came to an end, Max turned to Emily and smiled. \"Thanks for coming out to see us play,\" he said. \"We're glad you enjoyed it.\"\n",
      "\n",
      "Emily smiled back at him, feeling a spark of connection that she couldn't ignore. \"I loved it,\" she said. \"I'll come see you play again soon.\"\n",
      "\n",
      "Max nodded, and leaned in close. \"I'd like that,\" he said, his voice low and husky.\n",
      "\n",
      "As they walked out of the club, Emily felt a sense of excitement and possibility that she had never felt before. She knew that this was just the beginning of a new chapter in her life, one that would be filled with music, love, and adventure.\n"
     ]
    }
   ],
   "source": [
    "# ANSWER (set temperature = 0.2, do not have a max_completion_tokens setting)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model = \"llama-3.1-8b-instant\",\n",
    "    messages = [{\"role\": \"user\", \"content\": \"Continue the story: It was a great time to be alive\"}],\n",
    "    temperature = 0.2\n",
    ")\n",
    "    \n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "27aef18a-eee4-4d30-9de9-f97a303ad78d",
   "metadata": {
    "id": "27aef18a-eee4-4d30-9de9-f97a303ad78d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a great time to be alive, and Emma couldn't help but feel a sense of euphoria wash over her as she walked through the sun-kissed streets of 1960s San Francisco. The air was alive with the sounds of jazz music drifting from the clubs on Fillmore Street, and the smell of freshly brewed coffee wafted through the air from the quaint cafes that lined the way.\n",
      "\n",
      "As she strolled along, Emma's bright red dress catching the eye of every suited businessman who passed by, she felt a sense of freedom that she'd never known before. The war was over, and the world was finally beginning to open up to young people like her, full of hope and possibility.\n",
      "\n",
      "Emma had always been a rebel, drawn to the music and the art of the Beat Generation. She'd spent countless hours listening to Allen Ginsberg and Jack Kerouac read their poetry at City Lights Books, and she'd even tried her hand at writing her own verse, though it was still a secret she kept hidden from the world.\n",
      "\n",
      "As she turned a corner, Emma spotted a flyer tacked up on a lamppost. \"Join the Love-In\" it read, in bold, psychedelic letters. Emma's heart skipped a beat as she ripped off the flyer and stuffed it into her purse. She'd heard rumors about a massive gathering in Golden Gate Park, where thousands of young people would come together to promote peace and love.\n",
      "\n",
      "Without hesitation, Emma made her way to the bus stop, her mind racing with the possibilities. She knew it was a risk, but she'd always been drawn to the unknown, and this felt like the chance of a lifetime.\n",
      "\n",
      "As the bus rumbled down the highway, Emma felt a sense of excitement building inside her. She had no idea what the Love-In would be like, but she knew she had to be a part of it. Little did she know, her life was about to change in ways she never could have imagined.\n",
      "\n",
      "The bus pulled up, and Emma stepped aboard, her eyes shining with anticipation. She took a seat near the front, trying to process the thoughts racing through her mind. As the bus continued on its way, she leaned back and closed her eyes, a song playing in her head: \"All you need is love, love, love.\"\n",
      "\n",
      "The bus finally arrived at the park, and Emma joined the throngs of people making their way towards the gathering. She wandered through the crowds, taking in the sights and sounds of the Love-In. There were flower-power protesters, hippies with feathers in their hair, and countless others all united by one thing: a desire for peace and love.\n",
      "\n",
      "As Emma walked through the sea of flowers and brightly colored tents, she spotted a group of musicians setting up their equipment. She recognized the lead singer, a charismatic young man with a voice that sent shivers down her spine. His name was Jim, and he was from a band that had just moved to San Francisco from Los Angeles.\n",
      "\n",
      "Suddenly, Emma felt a tap on her shoulder. \"Hey, beautiful,\" a voice whispered in her ear. Emma turned to see Jim, flashing his famous smile. \"Want to join us on stage?\" he asked, his eyes sparkling with excitement.\n",
      "\n",
      "Emma's heart skipped a beat as she hesitated for what felt like an eternity. And then, in a split second, she made her decision. \"Yeah,\" she said, a mischievous grin spreading across her face. \"I'm in.\"\n",
      "\n",
      "With that, Emma's life was forever changed. She climbed the stairs to the stage, the crowd cheering and whistling as she took her place alongside Jim and his band. The music began, and Emma, the shy and reserved young woman from the suburbs, was transformed into a rock star, basking in the adoration of the crowd.\n",
      "\n",
      "As the night wore on, Emma found herself lost in the music, the energy of the crowd, and the excitement of the unknown. She had no idea what the future held, but she knew one thing for sure: it was a great time to be alive, and she was ready to face whatever came next.\n"
     ]
    }
   ],
   "source": [
    "# ANSWER (set temperature = 1, do not have a max_completion_tokens setting)\n",
    "response = client.chat.completions.create(\n",
    "    model = \"llama-3.1-8b-instant\",\n",
    "    messages = [{\"role\": \"user\", \"content\": \"Continue the story: It was a great time to be alive\"}],\n",
    "    temperature = 1\n",
    ")\n",
    "    \n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac38e059-e23a-44eb-92fc-3fec9c7bdcdf",
   "metadata": {
    "id": "ac38e059-e23a-44eb-92fc-3fec9c7bdcdf"
   },
   "source": [
    "Note what happens when the temperature is set too high!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "931f8e00-928c-4218-8e74-8c56269fbfcb",
   "metadata": {
    "id": "931f8e00-928c-4218-8e74-8c56269fbfcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a great time to be alive, and that fact wasn't lost on Jack Harris, a carefree 22-year-old who had just graduated from a thriving art school in San Francisco's vibrant Bay Area. The world was on the precipice of a creative revolution, led in part by Jack and his fellow creatives who saw opportunity at every street corner of this rapidly growing and ever-changing tech-obsessed city.\n",
      "\n",
      "The morning of the day we were getting started, the first rays peeks into Jack Harris's bright room on Potrero Hill at just before 05h00. That sun rise in such place was truly magical. But Jack could never sleep this morning, too excited. This city is full, always of life.\n"
     ]
    }
   ],
   "source": [
    "# ANSWER (set temperature = 2, do not have a max_completion_tokens setting)\n",
    "response = client.chat.completions.create(\n",
    "    model = \"llama-3.1-8b-instant\",\n",
    "    messages = [{\"role\": \"user\", \"content\": \"Continue the story: It was a great time to be alive\"}],\n",
    "    temperature = 2\n",
    ")\n",
    "    \n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e9bd9a-ad6b-409b-a751-023575868b6c",
   "metadata": {
    "id": "f4e9bd9a-ad6b-409b-a751-023575868b6c"
   },
   "source": [
    "### Zero-shot and one-short prompting for question-answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6662f0a-c7e3-4235-972d-2771781a4e53",
   "metadata": {
    "id": "f6662f0a-c7e3-4235-972d-2771781a4e53"
   },
   "source": [
    "This section shows the impact of prompting on the response. Zero-shot prompting means we provide the prompt without any examples or additional context. Let us initially ask Mistral a question using no prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e2d48674-a9cc-4e5a-8035-afcc37a01dfc",
   "metadata": {
    "id": "e2d48674-a9cc-4e5a-8035-afcc37a01dfc"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The reaction between two chemicals is determined by their chemical properties, such as their electronegativity, polarity, and reactivity. Here's a simplified overview of the process:\n",
       "\n",
       "1. **Atoms interact**: When two chemicals come into contact, their atoms interact through intermolecular forces such as London dispersion forces, dipole-dipole forces, and hydrogen bonding.\n",
       "2. **Molecules collide**: The atoms collide with each other, and if the collision is energetic enough, the molecules may break apart or react.\n",
       "3. **Reaction occurs**: If the molecules have the correct arrangement of electrons and atoms, a chemical reaction can occur. This is determined by the laws of thermodynamics, such as the law of conservation of energy and the law of conservation of matter.\n",
       "4. **Chemical bonds form or break**: During the reaction, chemical bonds are either formed or broken between the atoms. This can result in the creation of new compounds or the transformation of existing ones.\n",
       "5. **Energy is released or absorbed**: As the reaction occurs, energy is either released (exothermic reaction) or absorbed (endothermic reaction).\n",
       "\n",
       "Types of chemical reactions include:\n",
       "\n",
       "1. **Combustion reactions**: Oxygen reacts with a fuel to produce heat and light.\n",
       "Example: 2H2 + O2 → 2H2O\n",
       "2. **Dispersion reactions**: A chemical is dispersed into a solvent, often resulting in a solution.\n",
       "Example: NaCl + H2O → Na+ + Cl- + H2O\n",
       "3. **Acid-base reactions**: An acid reacts with a base to form a salt and water.\n",
       "Example: HCl + NaOH → NaCl + H2O\n",
       "4. **Oxidation-reduction reactions**: A chemical loses or gains electrons, resulting in a change in oxidation state.\n",
       "Example: 2Na + Cl2 → 2NaCl\n",
       "5. **Synthesis reactions**: Two or more chemicals combine to form a new compound.\n",
       "Example: 2H2 + O2 → 2H2O\n",
       "\n",
       "These are just a few examples of chemical reactions. The specific reaction between two chemicals depends on their chemical properties and the conditions under which they interact.\n",
       "\n",
       "**Factors influencing chemical reactions:**\n",
       "\n",
       "1. **Temperature**: Higher temperatures often increase the rate and yield of chemical reactions.\n",
       "2. **Pressure**: Increased pressure can increase the rate of some chemical reactions.\n",
       "3. **Concentration**: Higher concentrations of the reacting substances can increase the rate and yield of the reaction.\n",
       "4. **Catalysts**: Some substances can speed up or slow down chemical reactions without being consumed in the process.\n",
       "\n",
       "**Tools for predicting chemical reactions:**\n",
       "\n",
       "1. **Balancing equations**: Writing balanced chemical equations can help predict the products and stoichiometry of a reaction.\n",
       "2. **Reaction mechanisms**: Understanding the step-by-step process of a reaction can help predict the products and conditions required for the reaction to occur.\n",
       "3. **Thermodynamics**: Using thermodynamic principles, such as the Gibbs free energy equation, can help predict the spontaneity and direction of a reaction.\n",
       "\n",
       "Keep in mind that predicting chemical reactions can be complex and depends on various factors, including the specific conditions and substances involved."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"How do two chemicals react?\"}],\n",
    "    temperature = 0.8,\n",
    ")\n",
    "\n",
    "Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e7bac7-52f6-4779-a2a3-d45d29975f5d",
   "metadata": {
    "id": "c7e7bac7-52f6-4779-a2a3-d45d29975f5d"
   },
   "source": [
    "**Exercise:** Ask the same question but modify the prompt to return the answer to the same question in a simpler form (still using the llama-3.1-8b-instant model). Experiment with different prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "92250b60-9d68-4906-80b8-db78fc9c2ae6",
   "metadata": {
    "id": "92250b60-9d68-4906-80b8-db78fc9c2ae6"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Let's say we have two chemicals: Chemical A and Chemical B.\n",
       "\n",
       "**What happens when they meet?**\n",
       "\n",
       "Imagine Chemical A and Chemical B are two friends who want to interact with each other. They might not know each other at first, but when they meet, they start to talk and share stuff.\n",
       "\n",
       "**Chemical Reaction:**\n",
       "\n",
       "When Chemical A and Chemical B meet, they can do two things:\n",
       "\n",
       "1. **Be friends**: They can mix together and become a new, combined chemical. This is called a chemical reaction. It's like they've become a new team!\n",
       "2. **Change each other**: They can change each other's shape or form. This is also a chemical reaction. It's like they're wearing each other's hats!\n",
       "\n",
       "**Example:**\n",
       "\n",
       "* Chemical A might be a liquid (like water).\n",
       "* Chemical B might be a gas (like oxygen).\n",
       "* When they meet, they can form a new chemical (like water vapor).\n",
       "* Or, Chemical A might change into a solid (like ice) when it meets Chemical B.\n",
       "\n",
       "**Simple Example:**\n",
       "\n",
       "* Chemical A: Salt (sodium chloride)\n",
       "* Chemical B: Water\n",
       "* When they meet, they form a new chemical: Saltwater.\n",
       "\n",
       "That's it! Chemical reactions are like interactions between friends. They can mix and become new things, or change each other's shape or form.\n",
       "\n",
       "Do you have any questions?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ANSWER\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Explain how two chemicals react in very simple words, i am beginner\"}],\n",
    "    temperature=0.8,\n",
    ")\n",
    "\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04676a11-57a7-4f9c-87e2-128559f0ffce",
   "metadata": {
    "id": "04676a11-57a7-4f9c-87e2-128559f0ffce"
   },
   "source": [
    "### One-shot prompting ###\n",
    "\n",
    "Next, note the dramatic change when we give the following template setting a new role and providing an English question followed by a French translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fcb1c8fa-d3cb-400d-ae0f-3bc8f2d1d473",
   "metadata": {
    "id": "fcb1c8fa-d3cb-400d-ae0f-3bc8f2d1d473"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment deux composés chimiques réagissent-ils?\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"system\",\n",
    "             \"content\": \"You translate English to French.\"},\n",
    "              {\"role\": \"user\",\n",
    "               \"content\": \"What time is it?\"},\n",
    "               {\"role\": \"assistant\",\n",
    "               \"content\": \"Quelle heure est-il?\"},\n",
    "              {\"role\": \"user\",\n",
    "               \"content\": \"How do two chemicals react?\"}],\n",
    "    temperature = 0.8,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1394e5-34b9-46b8-aea6-87a7862b7269",
   "metadata": {
    "id": "fc1394e5-34b9-46b8-aea6-87a7862b7269"
   },
   "source": [
    "### Few-shot prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e3b3d0-c365-4138-9be0-9d324bc34050",
   "metadata": {
    "id": "d5e3b3d0-c365-4138-9be0-9d324bc34050"
   },
   "source": [
    "Recall that since the text generation process outputs one token at a time, their outputs often need adjusting. This is where examples can help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "68c6131b-3b30-45b0-8dab-0dc547033b6b",
   "metadata": {
    "id": "68c6131b-3b30-45b0-8dab-0dc547033b6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regrettably, I will be unable to attend the meeting. I apologize for any inconvenience this may cause.\n"
     ]
    }
   ],
   "source": [
    "prompt1 = \"I'm gonna head out now, see you later.\"\n",
    "response1 = \"I will be leaving now. See you later.\"\n",
    "\n",
    "prompt2 =  \"That movie was super cool!\"\n",
    "response2 = \"The movie was very impressive.\"\n",
    "\n",
    "prompt3 = \"Can't make it to the meeting, sorry.\"\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a professional editor. Rewrite casual sentences into a formal tone.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt1},\n",
    "        {\"role\": \"assistant\", \"content\": response1},\n",
    "        {\"role\": \"user\", \"content\": prompt2},\n",
    "        {\"role\": \"assistant\", \"content\": response2},\n",
    "        {\"role\": \"user\", \"content\": prompt3},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040e9e18-cc27-453b-8844-3f683fb607f8",
   "metadata": {
    "id": "040e9e18-cc27-453b-8844-3f683fb607f8"
   },
   "source": [
    "The output can also be moulded to provide SQL output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ed854534-30db-4745-b910-27d12a3ed47e",
   "metadata": {
    "id": "ed854534-30db-4745-b910-27d12a3ed47e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM products WHERE quantity = 0;\n"
     ]
    }
   ],
   "source": [
    "prompt1 = \"Show me all users who signed up in the last 30 days.\"\n",
    "response1 = \"SELECT * FROM users WHERE signup_date >= CURRENT_DATE - INTERVAL '30 days';\"\n",
    "\n",
    "prompt2 = \"What is the average order value?\"\n",
    "response2 =  \"SELECT AVG(order_total) FROM orders;\"\n",
    "\n",
    "prompt3 = \"List products that are out of stock.\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an assistant that translates natural language to SQL.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt1},\n",
    "        {\"role\": \"assistant\", \"content\": response1},\n",
    "        {\"role\": \"user\", \"content\": prompt2},\n",
    "        {\"role\": \"assistant\", \"content\": response2},\n",
    "        {\"role\": \"user\", \"content\": prompt3},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a1dda2-2913-4a59-bfc3-3f0a59f0dcc7",
   "metadata": {
    "id": "45a1dda2-2913-4a59-bfc3-3f0a59f0dcc7"
   },
   "source": [
    "**Exercise**: Create a few examples to train the \"llama3-70b-8192\" LLM to take in user content in the form below and provide output as a pandas dataframe. Use the `exec` function to execute its output to display the answer of sample input as a data frame.\n",
    "\n",
    "Example:\n",
    "\n",
    "given the user content\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "| col1 | col2 | col3\n",
    "\n",
    "| 32 | 27 | 25\n",
    "\n",
    "| 64 | 23 | 14\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "train the model to output\n",
    "\n",
    "df = pd.DataFrame({'col1': [32, 64], 'col2': [27, 23], 'col3': [25, 14]})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2b8ac08a-5759-41cd-a560-e77694573723",
   "metadata": {
    "id": "2b8ac08a-5759-41cd-a560-e77694573723"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code generated:\n",
      " print(df)\n",
      "\n",
      "\n",
      "Output:\n",
      "\n",
      "   col1  col2  col3\n",
      "0    32    27    25\n",
      "1    64    23    14\n"
     ]
    }
   ],
   "source": [
    "#ANSWER\n",
    "prompt1 = \"take in user content and provide output as a pandas dataframe.\"\n",
    "prompt2 = \"\"\"\n",
    "\n",
    "| col1 | col2 | col3 |\n",
    "\n",
    "| 32 | 27 | 25 |\n",
    "\n",
    "| 64 | 23 | 14 |\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt3 = \"\"\"df = pd.DataFrame({'col1': [32, 64],\n",
    "                   'col2': [27, 23],\n",
    "                   'col3': [25, 14]})\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": prompt1},\n",
    "        {\"role\": \"user\", \"content\": prompt2},\n",
    "        {\"role\": \"assistant\", \"content\": prompt3}\n",
    "    ]\n",
    ")\n",
    "\n",
    "output = response.choices[0].message.content.strip()\n",
    "print(\"Code generated:\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f62a88-c7b4-4431-acda-97e9a98981f4",
   "metadata": {
    "id": "29f62a88-c7b4-4431-acda-97e9a98981f4"
   },
   "source": [
    "Also show what happens when the question is asked in the absence of a system role and without few-shot prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "712221ea-14c0-4faa-a126-ca4f2f0538a6",
   "metadata": {
    "id": "712221ea-14c0-4faa-a126-ca4f2f0538a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code generated:\n",
      " You can use the `pd.DataFrame` constructor to create a Pandas DataFrame from the data:\n",
      "\n",
      "```\n",
      "import pandas as pd\n",
      "\n",
      "data = [[32, 27, 25], [64, 23, 14]]\n",
      "columns = ['col1', 'col2', 'col3']\n",
      "\n",
      "df = pd.DataFrame(data, columns=columns)\n",
      "\n",
      "print(df)\n",
      "```\n",
      "\n",
      "Output:\n",
      "\n",
      "```\n",
      "   col1  col2  col3\n",
      "0    32    27    25\n",
      "1    64    23    14\n",
      "```\n",
      "\n",
      "Alternatively, you can also use the `pd.DataFrame.from_records` method:\n",
      "\n",
      "```\n",
      "import pandas as pd\n",
      "\n",
      "data = [[32, 27, 25], [64, 23, 14]]\n",
      "columns = ['col1', 'col2', 'col3']\n",
      "\n",
      "df = pd.DataFrame.from_records(data, columns=columns)\n",
      "\n",
      "print(df)\n",
      "```\n",
      "\n",
      "Both methods will produce the same output.\n"
     ]
    }
   ],
   "source": [
    "# ANSWER\n",
    "prompt1 = \"\"\" Change the data to dataframe\n",
    "\n",
    "| col1 | col2 | col3 |\n",
    "\n",
    "| 32 | 27 | 25 |\n",
    "\n",
    "| 64 | 23 | 14 |\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt1}]\n",
    ")\n",
    "\n",
    "output = response.choices[0].message.content.strip()\n",
    "print(\"Code generated:\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28660884-dfd4-4b96-a65d-d21ddbf99423",
   "metadata": {
    "id": "28660884-dfd4-4b96-a65d-d21ddbf99423"
   },
   "source": [
    "### Chain-of-thought prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724f85ef-afc0-4e31-9ad8-ae780c535837",
   "metadata": {
    "id": "724f85ef-afc0-4e31-9ad8-ae780c535837"
   },
   "source": [
    "The results of question-answering can also be improved by prompting the LLM to provide intermediate steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee04ad6-968b-406a-8119-981b01ef5f92",
   "metadata": {
    "id": "bee04ad6-968b-406a-8119-981b01ef5f92"
   },
   "source": [
    "**Exercise**: Using the following prompts, compare the answers of the \"llama3-8b-8192\" model (set seed=21). (If this model is no longer available choose a model with relatively few parameters.)\n",
    "\n",
    "zero_shot_prompt = \"How many s's are in the word 'success'?\"\n",
    "\n",
    "chain_of_thought_prompt = \"How many s's are in the word 'success'? Explain your answer step by step by going through each letter in turn.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bc8868f2-7c4f-4b99-bbd2-cc731cca74bb",
   "metadata": {
    "id": "bc8868f2-7c4f-4b99-bbd2-cc731cca74bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Model Name                                                                                                        Question                                                                                                                                                                                                                                                                                                                  Answer\n",
      "0  llama3-8b-8192                                                                         How many s's are in the word 'success'?                                                                                                                                                                                                                                                                                  There are 2 s's in the word \"success\".\n",
      "1  llama3-8b-8192  How many s's are in the word 'success'? Explain your answer step by step by going through each letter in turn.  Let's go through each letter of the word \"success\" to count the number of s's:\\n\\n1. S (first letter)\\n2. U\\n3. C\\n4. C\\n5. E\\n6. S\\n7. S\\n\\nAs we go through the letters, we find two S's (at positions 1 and 6) and three other letters. Therefore, the correct answer is:\\n\\nThere are 2 S's in the word \"success\".\n"
     ]
    }
   ],
   "source": [
    "# ANSWER\n",
    "prompt = [ \n",
    "    \"How many s's are in the word 'success'?\",\n",
    "    \"How many s's are in the word 'success'? Explain your answer step by step by going through each letter in turn.\"\n",
    "]\n",
    "\n",
    "model = \"llama3-8b-8192\"\n",
    "\n",
    "results = []\n",
    "\n",
    "# Looping each model + question\n",
    "for p in prompt: \n",
    "        response = client.chat.completions.create(\n",
    "            model= model,\n",
    "            messages=[{\"role\": \"user\", \"content\": p}],\n",
    "            max_tokens=300,\n",
    "            seed = 21\n",
    "        )\n",
    "        \n",
    "        answer = response.choices[0].message.content.strip()\n",
    "        \n",
    "        results.append({\n",
    "            \"Model Name\": model,\n",
    "            \"Question\": p,\n",
    "            \"Answer\": answer\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09499b6e-a4aa-439c-a785-3f0e06a3ba4f",
   "metadata": {
    "id": "09499b6e-a4aa-439c-a785-3f0e06a3ba4f"
   },
   "source": [
    "## Comparison of LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0e82d9-cefc-4af3-9d95-5e22db6cd56f",
   "metadata": {
    "id": "dd0e82d9-cefc-4af3-9d95-5e22db6cd56f"
   },
   "source": [
    "**Exercise**: Compare the performance of 2 LLMs by outputting the answers of the following questions into a dataframe.\n",
    "\n",
    "    \"Tell me a joke about data science.\",\n",
    "    \"How can one calculate 22 * 13 mentally?\",\n",
    "    \"Write a creative story about a baby learning to crawl.\",\n",
    "\n",
    "Column headings:\n",
    "\n",
    "Model Name | Question | Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b5e0c587-9650-4082-baf4-d9cdd94bc238",
   "metadata": {
    "id": "b5e0c587-9650-4082-baf4-d9cdd94bc238"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Model Name                                                Question                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Answer\n",
      "0          gemma2-9b-it                      Tell me a joke about data science.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Why did the data scientist break up with the statistician? \\n\\nBecause they had too many differences in their distributions! 😂  \\n\\n\\nLet me know if you'd like to hear another one!\n",
      "1          gemma2-9b-it                 How can one calculate 22 * 13 mentally?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Here's how you can calculate 22 * 13 mentally using a combination of rounding and estimation:\\n\\n**1. Round:**\\n\\n*  Round 22 to 20 (a bit easier to work with).\\n*  Round 13 to 10.\\n\\n**2.  Estimate:**\\n\\n* 20 * 10 = 200\\n\\n**3.  Adjust:**\\n\\n*  You rounded up 22 to 20, so the answer will be slightly lower than 200.\\n*  You rounded down 13 to 10, so the answer will be slightly higher than 200.\\n\\n**4.  Consider the difference:**\\n\\n*  22 is 2 more than 20.\\n*  13 is 3 more than 10.\\n\\n*  A rough estimate would be 200 + (2 * 10) ≈ 220. Then adjust slightly higher due to the rounding down of 13.\\n\\n\\n**Final  Thoughts:**\\n\\nThis method gives you a good mental estimate.  For a precise answer, you'd want to do the full calculation.\n",
      "2          gemma2-9b-it  Write a creative story about a baby learning to crawl.  Bartholomew \"Barty\" Bumblebottom was determined. He wasn't content to merely observe the world from his plush-padded kingdom, his highchair throne. He wanted to join the exciting chaos below, the dizzying world of dust bunny pursuit and sunbeam chases. He wanted to crawl.\\n\\nHis older sister, Beatrice, a seasoned veteran of the crawling wars, eyed him with amusement. \"It's not as easy as it looks, little Barty,\" she'd say, scooting sideways, leaving a trail of giggling and a scattering of building blocks in her wake.\\n\\nBarty, however, was undeterred. He'd spend hours peering over the edge of his highchair, mimicking Beatrice's movements with wobbly arms and determined gurgles. He'd practice pushing off with his chubby legs,  his tiny toes curling like miniature ballerinas.\\n\\nOne sunny afternoon, with the aroma of honey sandwiches wafting through the air, Barty spotted his prize: a particularly vibrant, lopsided sunbeam that danced across the rug, promising untold adventure. He nestled his hands and knees on the cushioned floor, his resolve hardening like a pretzel.\\n\\nHe kicked out a leg, wobbled alarmingly, then pushed again, harder this time. His toes scrabbled for purchase, and for a heart-stopping moment, his entire world seemed to tilt. But then, a miracle! He rolled forward, his little legs churning,\n",
      "3  llama-3.1-8b-instant                      Tell me a joke about data science.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Why did the data scientist break up with his girlfriend? \\n\\nBecause he realized he was in a correlation, but not a causation - he liked the data, but he didn't understand her behavior.\n",
      "4  llama-3.1-8b-instant                 How can one calculate 22 * 13 mentally?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                To calculate 22 * 13 mentally, you can break down the problem into simpler steps:\\n\\n1. First, multiply 20 and 13: \\n   20 * 13 = 260\\n\\n2. Now, consider the additional 2 * 13:\\n   2 * 10 = 20\\n   2 * 3 = 6\\n   Add the results together: 20 + 6 = 26\\n\\n3. Finally, add the results from step 1 and step 2:\\n   260 + 26 = 286\\n\\nTherefore, the answer is 286.\n",
      "5  llama-3.1-8b-instant  Write a creative story about a baby learning to crawl.                                                                                    In a cozy little house on a sunny street, a bundle of joy was stirring in her crib. Emma, a chubby-cheeked baby with a mop of curly brown hair, was learning to crawl. She had been practicing for weeks, but today was the day she would make her big move.\\n\\nAs her mom, Sarah, sat on the floor, watching with a warm smile, Emma began to rock back and forth in her crib. \"Wheeee!\" she cooed, flapping her fists in excitement. This was it, the moment of truth. Sarah's eyes sparkled as she whispered encouragement, \"You got this, sweetie!\"\\n\\nEmma's legs started to quiver, then wobble, as she strained to push herself up. Her mom held her little hands, guiding her feet onto the floor. For a moment, Emma just sat there, staring at her hands and feet, as if trying to remember what to do next. Then, with a burst of confidence, she began to scoot forward, inch by inch, her arms flailing wildly.\\n\\nSarah's voice grew louder, urging Emma on, \"You're doing it! Keep going!\" Emma giggled, oblivious to her mom's enthusiasm, as she made her way across the floor. She bumped into a toy block, then a pillow, but didn't miss a beat. Every time she stumbled, Sarah was there to catch her, and Emma would giggle some more.\\n\\nAs she crawled, Emma discovered\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None) # allows wide dataframes to be viewed\n",
    "models = [\"gemma2-9b-it\", \"llama-3.1-8b-instant\"] #can edit this\n",
    "\n",
    "# ANSWER\n",
    "prompt = [ \n",
    "    \"Tell me a joke about data science.\",\n",
    "    \"How can one calculate 22 * 13 mentally?\",\n",
    "    \"Write a creative story about a baby learning to crawl.\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Looping each model + question\n",
    "for model in models:\n",
    "    for p in prompt: \n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": p}],\n",
    "            max_tokens=300\n",
    "        )\n",
    "        \n",
    "        answer = response.choices[0].message.content.strip()\n",
    "        \n",
    "        results.append({\n",
    "            \"Model Name\": model,\n",
    "            \"Question\": p,\n",
    "            \"Answer\": answer\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3e2651-0bee-40bd-b5cc-3c2891b1adb5",
   "metadata": {
    "id": "8f3e2651-0bee-40bd-b5cc-3c2891b1adb5"
   },
   "source": [
    "### Bonus\n",
    "\n",
    "See if you can prompt an LLM to perform sentiment analysis (output 'Positive' or 'Negative' only) on a given piece of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d7169fd6-f1d1-4a5d-8fd9-21167a54416a",
   "metadata": {
    "id": "d7169fd6-f1d1-4a5d-8fd9-21167a54416a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                            Text  \\\n",
      "0  I love to solve data science issue but i find difficulties to understand many techniques behind data science.   \n",
      "\n",
      "  Sentiment  \n",
      "0  Negative  \n"
     ]
    }
   ],
   "source": [
    "# ANSWER\n",
    "prompt1 = (\"Please help to classifier this statement based on sentiment analysis.\"\n",
    "            \"Do not output explanation and response only 'Positive or Negative'.\")\n",
    "prompt2 = [\"I love to solve data science issue but i find difficulties to understand many techniques behind data science.\"]\n",
    "\n",
    "model = \"llama3-8b-8192\"\n",
    "\n",
    "rows = []\n",
    "for t in prompt2:\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt1},\n",
    "            {\"role\": \"user\", \"content\": t}\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_completion_tokens=5,\n",
    "    )\n",
    "    sentiment = resp.choices[0].message.content.strip()\n",
    "    rows.append({\"Text\": t, \"Sentiment\": sentiment})\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0387259e-0bf8-400e-97d9-d3f5688b2e4d",
   "metadata": {
    "id": "0387259e-0bf8-400e-97d9-d3f5688b2e4d"
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f279421d-cbca-4003-941e-c2d2bc2833a8",
   "metadata": {
    "id": "f279421d-cbca-4003-941e-c2d2bc2833a8"
   },
   "source": [
    "We worked with a few Large Language Models (LLMs) using Groq and experimented with prompting for summarisation, text completion and question-answering tasks.\n",
    "\n",
    "We also explored controlling the randomness (creativity) of output through the temperature setting and tried different types of prompting to achieve desired forms of output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f06d5e-7073-485b-b046-afe839ab844c",
   "metadata": {
    "id": "94f06d5e-7073-485b-b046-afe839ab844c"
   },
   "source": [
    "## References\n",
    "1. [Groq's prompting guide](https://console.groq.com/docs/prompting)\n",
    "2. [Groq's playground](https://console.groq.com/playground)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61aab4a-1330-4762-9318-5635c3a97aa7",
   "metadata": {
    "id": "d61aab4a-1330-4762-9318-5635c3a97aa7"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "> > > > > > > > > © 2025 Institute of Data\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
